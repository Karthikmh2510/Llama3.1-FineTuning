# -*- coding: utf-8 -*-
"""MedTerm_FineTune_Llama-3.1-8B-Instruct.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q-3LCGYXtWyX4Nl_x7MVAIAftEc228vB
"""

!pip install bitsandbytes peft trl accelerate datasets transformers huggingface_hub

import torch, transformers, os
from datasets import load_dataset
from google.colab import userdata
from peft import LoraConfig
from trl import SFTTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig, LlamaForCausalLM

os.environ['HF_Finegrained_Token'] = userdata.get('HF_Finegrained_Token')
os.environ['HF_Write_Token'] = userdata.get('HF_Write_Token')

"""# Pre-req"""

# Quantization
model_id = "meta-llama/Llama-3.1-8B-Instruct"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, # Default Gemma in 32 to 4-bit conversion
    bnb_4bit_compute_dtype=torch.bfloat16, # FineTuning weights converts to float 16-bit to make-
            # balance b/w loading gemma model in 4-bit while retaining info and quality in converting-
            # finetuned model weight to be held in 16-bits.
    bnb_4bit_quant_type="nf4"
)

tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_Finegrained_Token'])

model = AutoModelForCausalLM.from_pretrained(model_id,
                                             quantization_config=bnb_config,
                                             token=os.environ['HF_Finegrained_Token'],
                                             device_map={"":0},
                                             )

"""# Inferencing the model"""

# Test 1
text = "What is Paracetamol poisoning?"
device = "cuda:0"
inputs = tokenizer(text, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

os.environ["WANDB_DISABLED"] = "false"

lora_config = LoraConfig(
    r=16,
    target_modules=["q_proj", "o_proj", "k_proj", "v_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

dataset = load_dataset("dmedhi/wiki_medical_terms", split="train")

# Reformat dataset to include a "text" field
def format_data(entry):
    return {
        "text": f"Question: {entry['medical_term']}\nAnswer: {entry['wiki_description']}"
    }

# Apply formatting
formatted_dataset = dataset.map(format_data)

trainer = SFTTrainer(
    model=model,
    train_dataset=formatted_dataset,
    args = transformers.TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        num_train_epochs=3,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=1,
        output_dir="outputs",
        optim="paged_adamw_8bit"
    ),
    peft_config=lora_config,
  )

trainer.train()

model.save_pretrained("Karthik2510/Medi_terms_Llama3.1_8B_instruct_model")

"""**Old Response:**
What is Paracetamol poisoning? What are its symptoms and treatment?
Paracetamol (also known as acetaminophen) is a commonly used over-the-counter pain reliever and fever reducer. However, taking too much of it can cause serious health problems, including liver damage and death. Paracetamol poisoning can occur when someone takes more than the recommended dose or takes it more frequently than directed.
Symptoms of Paracetamol poisoning:
The symptoms of paracetamol poisoning can be mild, moderate, or severe, and they can appear anywhere from 2-72 hours after taking the overdose.
Mild symptoms:
Moderate symptoms:
Severe symptoms:
Treatment of Paracetamol poisoning:
If you suspect someone has taken an overdose of paracetamol, call your local emergency number or the national poison control center (1-800-222-1222 in the US) for guidance. Treatment may involve:
Activated charcoal: Administering activated charcoal by mouth to help absorb the paracetam

**New Response:**

Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
What is Paracetamol poisoning? Paracetamol poisoning, also known as acetaminophen poisoning, is an overdose of paracetamol (acetaminophen) that results in harmful and life-threatening side effects. Paracetamol poisoning can occur accidentally or as an attempt to commit suicide. It is the most common cause of acute liver failure in the Western world. Paracetamol is one of the most commonly used medications in the developed world and is safe when used as directed. Paracetamol is sold under many brand names, including Tylenol and Panadol. The medication is used to treat fever and mild to moderate pain. It is also used as a recreational drug, known as a "lethal dose" or "overdose" in some circles, though it is considered to be one of the safer recreational drugs, due to its relatively low risk of death, in comparison to other commonly used recreational drugs. It is also used as an alternative to opioids in cancer and post-operative pain.
"""

# Test 1
text = "What is Paracetamol poisoning?"
device = "cuda:0"
inputs = tokenizer(text, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from huggingface_hub import notebook_login
notebook_login()
import os
from google.colab import userdata
HF_WRITE_TOKEN=userdata.get('HF_Write_Token')

model.push_to_hub("Karthik2510/Medi_terms_Llama3_1_8B_instruct_model", token=HF_WRITE_TOKEN)



"""# Testing the model"""

# BIOSSES (Biomedical Sentence Similarity)
# Use case: Evaluates semantic similarity between medical texts.
# Why? Good for embedding model evaluations.

# dataset_BIOSSES = load_dataset("BIOSSES")
dataset_BIOSSES = load_dataset("BIOSSES", split="train")  # Select "train" split explicitly

print(dataset[0])  # View a sentence pair

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load your model
model_name = "Karthik2510/Medi_terms_Llama3_1_8B_instruct_model"

model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")

from sentence_transformers import SentenceTransformer, util

# Load sentence similarity model
sim_model = SentenceTransformer("all-MiniLM-L6-v2")  # Efficient similarity model

# Assuming tokenizer is your instance of PreTrainedTokenizerFast
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
model.resize_token_embeddings(len(tokenizer))

# Function to generate responses
def generate_response(sentence):
    inputs = tokenizer(sentence, return_tensors="pt", truncation=True, padding=True).to("cuda")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

#  Function to compute similarity between model output and reference sentence
def compute_similarity(sentence1, sentence2):
    emb1 = sim_model.encode(sentence1, convert_to_tensor=True)
    emb2 = sim_model.encode(sentence2, convert_to_tensor=True)
    return util.pytorch_cos_sim(emb1, emb2).item()

# Evaluate model on the BIOSSES dataset
similarity_scores = []
num_samples = min(10, len(dataset_BIOSSES))  # Adjust to avoid exceeding dataset size

for i in range(num_samples):
    s1 = dataset[i]["sentence1"]
    s2 = dataset[i]["sentence2"]

    # Generate response using the model
    generated_text = generate_response(s1)

    # Compute similarity
    similarity = compute_similarity(generated_text, s2)
    similarity_scores.append(similarity)

    # Print results for each test case
    print(f"üîπ Example {i+1}:")
    print(f"Original Sentence  : {s1}")
    print(f"Generated Sentence : {generated_text}")
    print(f"Reference Sentence : {s2}")
    print(f"Similarity Score   : {similarity:.4f}\n")

# Compute and print average similarity score
average_similarity = sum(similarity_scores) / len(similarity_scores)
print(f"üèÜ Model's Average Similarity Score on BIOSSES: {average_similarity:.4f}")